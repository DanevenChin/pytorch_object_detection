{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import os\n",
    "from pycocotools.coco import COCO\n",
    "import cv2\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubtractMeans(object):\n",
    "    def __init__(self, mean):\n",
    "        self.mean = np.array(mean, dtype=np.float32)\n",
    "\n",
    "    def __call__(self, image, boxes = None, labels = None):\n",
    "        image = image.astype(np.float32)\n",
    "        image -= self.mean\n",
    "        return image.astype(np.float32), boxes, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resize(object):\n",
    "    def __init__(self, Resize = 512):\n",
    "        self.Resize = Resize\n",
    "\n",
    "    def __call__(self, image, boxes = None, labels = None):\n",
    "        image = cv2.resize(image, (self.Resize, self.Resize))\n",
    "        return image, boxes, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvertToFloat(object):\n",
    "    \"\"\"convert image from int to float\n",
    "    Args:\n",
    "        image: numpy array \n",
    "    \"\"\"\n",
    "    def __call__(self, image, boxes = None, labels = None):\n",
    "        return image.astype(np.float32), boxes, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Compose(object):\n",
    "    \"\"\"Composes several augmentations together.\n",
    "    Args:\n",
    "        transforms (List[Transform]): list of transforms to compose.\n",
    "    Example:\n",
    "        >>> augmentations.Compose([\n",
    "        >>>     transforms.CenterCrop(10),\n",
    "        >>>     transforms.ToTensor(),\n",
    "        >>> ])\n",
    "    \"\"\"\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, img, boxes = None, labels = None):\n",
    "        for transform in self.transforms:\n",
    "            img, boxes, labels = transform(img, boxes, labels)\n",
    "        return img, boxes, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSDAugmentation(object):\n",
    "    def __init__(self, imgSize = 512, mean = (0, 0, 0)):\n",
    "        self.mean = mean\n",
    "        self.imgSize = imgSize\n",
    "        self.augment = Compose([\n",
    "            ConvertToFloat(),\n",
    "            Resize(self.imgSize),\n",
    "            SubtractMeans(self.mean)\n",
    "        ])\n",
    "\n",
    "    def __call__(self, img, boxes, labels):\n",
    "        return self.augment(img, boxes, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class COCOAnnotationTransform(object):\n",
    "    \"\"\"Transforms a COCO annotation into a Tensor of bbox coords and label index\n",
    "    Initilized with a dictionary lookup of classnames to indexes\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.label_map = self._get_label_map('/Users/kehwaweng/Documents/ObjectDetection/torch_ssd_mobilenet/coco_labels.txt')\n",
    "\n",
    "    def __call__(self, target, width, height):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            target (dict): COCO target json annotation as a python dict\n",
    "            height (int): height\n",
    "            width (int): width\n",
    "        Returns:\n",
    "            a list containing lists of bounding boxes  [bbox coords, class idx]\n",
    "        \"\"\"\n",
    "        scale = np.array([width, height, width, height])\n",
    "        res = []\n",
    "        for obj in target:\n",
    "            if 'bbox' in obj:\n",
    "                bbox = obj['bbox']\n",
    "                bbox[2] += bbox[0]\n",
    "                bbox[3] += bbox[1]\n",
    "                label_idx = self.label_map[obj['category_id']] - 1\n",
    "                final_box = list(np.array(bbox)/scale)\n",
    "                final_box.append(label_idx)\n",
    "                res += [final_box]  # [xmin, ymin, xmax, ymax, label_idx]\n",
    "            else:\n",
    "                print(\"no bbox problem!\")\n",
    "        return res \n",
    "    \n",
    "    def _get_label_map(self, label_file):\n",
    "        label_map = {}\n",
    "        labels = open(label_file, 'r')\n",
    "        for line in labels:\n",
    "            ids = line.split(',')\n",
    "            label_map[int(ids[0])] = int(ids[1])\n",
    "        return label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class COCODetection(Dataset):\n",
    "    def __init__(self, image_set = \"train2017\", transform = SSDAugmentation(), target_transform = COCOAnnotationTransform()):\n",
    "        self.root = os.path.join(\"/Volumes/IPEVO_X0244/coco_dataset/\",image_set)\n",
    "        self.coco = COCO(annotation_file= os.path.join(\"/Volumes/IPEVO_X0244/coco_dataset/annotations_2017\",\n",
    "                                                       \"instances_{}.json\".format(image_set)))\n",
    "        self.ids = list(self.coco.imgToAnns.keys())\n",
    "        self.transforms = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            idx (int): Index\n",
    "        Returns:\n",
    "            tuple: Tuple (image, target).\n",
    "                   target is the object returned by coco.loadAnns\n",
    "        \"\"\"\n",
    "        img, gt, h, w = self.pull_item(idx)\n",
    "        return img, gt\n",
    "    \n",
    "    def __len__(self): \n",
    "        return len(self.ids)\n",
    "    \n",
    "    def pull_item(self, idx):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            idx (int): Index\n",
    "        Returns:\n",
    "            tuple: tuple (image, target, width, height)\n",
    "                    target is the object returned by coco.loadAnns\n",
    "        \"\"\"\n",
    "        img_id = self.ids[idx]\n",
    "        target = self.coco.imgToAnns[img_id]\n",
    "#         ann_ids = self.coco.getAnnIds(img_id)\n",
    "#         target = self.coco.loadAnns(ann_ids)\n",
    "        img_path = os.path.join(\"/Volumes/IPEVO_X0244/coco_dataset/train2017/\", self.coco.loadImgs(img_id)[0]['file_name'])\n",
    "        assert os.path.exists(img_path), \"loading image error\"\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        height, width, _ = img.shape\n",
    "        \n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target, width, height)\n",
    "        if self.transform is not None:\n",
    "            target = np.array(target)\n",
    "            img, boxes, labels = self.transform(img, \n",
    "                                                target[:, :4],\n",
    "                                                target[:, 4])\n",
    "            # to rgb\n",
    "            img = img[:, :, (2, 1, 0)]\n",
    "\n",
    "            target = np.hstack((boxes, np.expand_dims(labels, axis=1)))\n",
    "        \n",
    "        return torch.from_numpy(img.transpose(2, 0, 1)), target, height, width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=36.62s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "dataset = COCODetection()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detection_collate(batch):\n",
    "    \"\"\"Custom collate fn for dealing with batches of images that have a different\n",
    "    number of associated object annotations (bounding boxes).\n",
    "    Arguments:\n",
    "        batch: (tuple) A tuple of tensor images and lists of annotations\n",
    "    Return:\n",
    "        A tuple containing:\n",
    "            1) (tensor) batch of images stacked on their 0 dim\n",
    "            2) (list of tensors) annotations for a given image are stacked on\n",
    "                                 0 dim\n",
    "    \"\"\"\n",
    "    targets = []\n",
    "    imgs = []\n",
    "    for sample in batch:\n",
    "        imgs.append(sample[0])\n",
    "        targets.append(torch.FloatTensor(sample[1]))\n",
    "    return torch.stack(imgs, 0), targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(dataset= dataset, batch_size = 6, shuffle= True, collate_fn= detection_collate)\n",
    "\n",
    "for img, target in loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3, 512, 512])\n",
      "torch.Size([2, 5])\n",
      "torch.Size([37, 5])\n",
      "torch.Size([3, 5])\n",
      "torch.Size([3, 5])\n",
      "torch.Size([18, 5])\n",
      "torch.Size([14, 5])\n"
     ]
    }
   ],
   "source": [
    "print(img.shape)\n",
    "for i in target:\n",
    "    print(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mobilenet v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_divisible(v, divisor, min_value=None):\n",
    "    \"\"\"\n",
    "    This function is taken from the original tf repo.\n",
    "    It ensures that all layers have a channel number that is divisible by 8\n",
    "    It can be seen here:\n",
    "    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n",
    "    :param v:\n",
    "    :param divisor:\n",
    "    :param min_value:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if min_value is None:\n",
    "        min_value = divisor\n",
    "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
    "    # Make sure that round down does not go down by more than 10%.\n",
    "    if new_v < 0.9 * v:\n",
    "        new_v += divisor\n",
    "    return new_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvbBNReLU(in_channels, out_channels, stride, use_batch_norm = True):\n",
    "    if use_batch_norm:\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels= in_channels,\n",
    "                      out_channels= out_channels,\n",
    "                      kernel_size= 3,\n",
    "                      stride= stride,\n",
    "                      padding= 1,\n",
    "                      bias= False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace = False)\n",
    "        )\n",
    "    else:\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels= in_channels, \n",
    "                      out_channels= out_channels,\n",
    "                      kernel_size= 3,\n",
    "                      stride= stride,\n",
    "                      padding= 1,\n",
    "                      bias= False),\n",
    "            nn.ReLU(inplace= False)\n",
    "        )\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_1x1_bn(in_channels, out_channels, use_batch_norm = True):\n",
    "    if use_batch_norm:\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, \n",
    "                      out_channels, \n",
    "                      kernel_size = 1,\n",
    "                      stride = 1,\n",
    "                      padding = 0,\n",
    "                      bias = False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace = False)\n",
    "        )\n",
    "    else:\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, \n",
    "                      out_channels, \n",
    "                      kernel_size = 1,\n",
    "                      stride = 1,\n",
    "                      padding = 0,\n",
    "                      bias = False),\n",
    "            nn.ReLU(inplace = False)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvertedResidual(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride, expand_ratio, use_batch_norm = True):\n",
    "        super(InvertedResidual, self).__init__()\n",
    "\n",
    "        self.stride = stride\n",
    "        assert stride in [1, 2]\n",
    "\n",
    "        hidden_dim = round(in_channels * expand_ratio)\n",
    "        self.use_res_connect = self.stride == 1 and in_channels == out_channels\n",
    "\n",
    "        if expand_ratio == 1:\n",
    "            if use_batch_norm:\n",
    "                self.conv = nn.Sequential(\n",
    "                    # dw\n",
    "                    nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups = hidden_dim, bias = False),\n",
    "                    nn.BatchNorm2d(hidden_dim),\n",
    "                    nn.ReLU(inplace = False),\n",
    "                    # pw-linear\n",
    "                    nn.Conv2d(hidden_dim, out_channels, 1, 1, 0, bias = False),\n",
    "                    nn.BatchNorm2d(out_channels),\n",
    "                )\n",
    "            else:\n",
    "                self.conv = nn.Sequential(\n",
    "                    # dw\n",
    "                    nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups = hidden_dim, bias = False),\n",
    "                    nn.ReLU(inplace = False),\n",
    "                    # pw-linear\n",
    "                    nn.Conv2d(hidden_dim, out_channels, 1, 1, 0, bias = False),\n",
    "                )\n",
    "        else:\n",
    "            if use_batch_norm:\n",
    "                self.conv = nn.Sequential(\n",
    "                    # pw\n",
    "                    nn.Conv2d(in_channels, hidden_dim, 1, 1, 0, bias = False),\n",
    "                    nn.BatchNorm2d(hidden_dim),\n",
    "                    nn.ReLU(inplace = False),\n",
    "                    # dw\n",
    "                    nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups = hidden_dim, bias = False),\n",
    "                    nn.BatchNorm2d(hidden_dim),\n",
    "                    nn.ReLU(inplace = False),\n",
    "                    # pw-linear\n",
    "                    nn.Conv2d(hidden_dim, out_channels, 1, 1, 0, bias = False),\n",
    "                    nn.BatchNorm2d(out_channels),\n",
    "                )\n",
    "            else:\n",
    "                self.conv = nn.Sequential(\n",
    "                    # pw\n",
    "                    nn.Conv2d(in_channels, hidden_dim, 1, 1, 0, bias = False),\n",
    "                    nn.ReLU(inplace = False),\n",
    "                    # dw\n",
    "                    nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups = hidden_dim, bias = False),\n",
    "                    nn.ReLU(inplace = False),\n",
    "                    # pw-linear\n",
    "                    nn.Conv2d(hidden_dim, out_channels, 1, 1, 0, bias = False),\n",
    "                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_res_connect:\n",
    "            return x + self.conv(x)\n",
    "        else:\n",
    "            return self.conv(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mobilenetv2(nn.Module):\n",
    "    def __init__(self, \n",
    "                 n_classes = 80,\n",
    "                 image_size = 512,\n",
    "                 width_mult = 1.,\n",
    "                 round_nearest = 8, \n",
    "                 dropout_ratio = 0.2,\n",
    "                 use_batch_norm = True,):\n",
    "        super(mobilenetv2, self).__init__()\n",
    "        \n",
    "        assert image_size % 32 == 0\n",
    "        image_channels = 3\n",
    "        \n",
    "        last_channel = 1280\n",
    "        input_channel = 32\n",
    "        input_channel = _make_divisible(input_channel * width_mult, round_nearest)\n",
    "        self.last_channel = _make_divisible(last_channel * max(1.0, width_mult), round_nearest)\n",
    "        \n",
    "        inverted_residual_setting = [\n",
    "            #expand_ratio, channel, number of residual bloc, stride\n",
    "            [1, 16, 1, 1],\n",
    "            [6, 24, 2, 2],\n",
    "            [6, 32, 3, 2],\n",
    "            [6, 64, 4, 2],\n",
    "            [6, 96, 3, 1],\n",
    "            [6, 160, 3, 2],\n",
    "            [6, 320, 1, 1],\n",
    "        ]\n",
    "        # only check the first element, assuming user knows t,c,n,s are required\n",
    "        if len(inverted_residual_setting) == 0 or len(inverted_residual_setting[0]) != 4:\n",
    "            raise ValueError(\"inverted_residual_setting should be non-empty \"\n",
    "                             \"or a 4-element list, got {}\".format(inverted_residual_setting))\n",
    "        \n",
    "        self.extract_feature = []\n",
    "        self.extract_feature.append( ConvbBNReLU(image_channels, input_channel, stride = 2))\n",
    "        \n",
    "        for t, c, n, s in inverted_residual_setting:\n",
    "            output_channel = _make_divisible(c * width_mult, round_nearest)\n",
    "            for i in range(n):\n",
    "                stride = s if i == 0 else 1\n",
    "                self.extract_feature.append(InvertedResidual(in_channels = input_channel,\n",
    "                                                             out_channels = output_channel,\n",
    "                                                             stride = stride, \n",
    "                                                             expand_ratio = t,\n",
    "                                                             use_batch_norm = use_batch_norm\n",
    "                                                             )\n",
    "                                           )\n",
    "                input_channel = output_channel\n",
    "                \n",
    "        self.extract_feature.append(conv_1x1_bn(in_channels = input_channel, \n",
    "                                         out_channels = self.last_channel,\n",
    "                                         use_batch_norm = use_batch_norm))\n",
    "        # make it nn.Sequential\n",
    "        self.extract_feature = nn.Sequential(*self.extract_feature)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(self.last_channel, n_classes),\n",
    "        )\n",
    "    \n",
    "    \n",
    "        # weight initialization\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.ones_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.zeros_(m.bias)    \n",
    "                \n",
    "    def forward(self, x):\n",
    "        x = self.extract_feature(x)\n",
    "        x = x.mean(3).mean(2)\n",
    "        x = self.classifier(x)\n",
    "        return x       \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = mobilenetv2()\n",
    "# summary(model, (3,512,512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SeperableConv2d(in_channels, out_channels, kernel_size = 1, stride = 1, padding = 0):\n",
    "    \"\"\"Replace Conv2d with a depthwise Conv2d and Pointwise Conv2d.\n",
    "    \"\"\"\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels = in_channels,\n",
    "                  out_channels = in_channels,\n",
    "                  kernel_size = kernel_size,\n",
    "                  groups = in_channels,\n",
    "                  stride = stride,\n",
    "                  padding = padding),\n",
    "        nn.BatchNorm2d(in_channels),\n",
    "        nn.ReLU(inplace = False),\n",
    "        nn.Conv2d(in_channels = in_channels, \n",
    "                  out_channels = out_channels,\n",
    "                  kernel_size = 1),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride,):\n",
    "        super(Conv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size= 1, stride= stride, bias= False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace= False)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class liteConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride):\n",
    "        super(liteConv, self).__init__()\n",
    "\n",
    "        hidden_dim = out_channels // 2        \n",
    "        self.conv = nn.Sequential(\n",
    "            # pw\n",
    "            nn.Conv2d(in_channels, hidden_dim, 1, 1, 0, bias = False),\n",
    "            nn.BatchNorm2d(hidden_dim),\n",
    "            nn.ReLU(inplace = False),\n",
    "            # dw\n",
    "            nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups = hidden_dim, bias = False),\n",
    "            nn.BatchNorm2d(hidden_dim),\n",
    "            nn.ReLU(inplace = False),\n",
    "            # pw-linear\n",
    "            nn.Conv2d(hidden_dim, out_channels, 1, 1, 0, bias = False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace = False)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class liteConv_2(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride):\n",
    "        super(liteConv_2, self).__init__()\n",
    "\n",
    "        hidden_dim = out_channels // 2        \n",
    "        \n",
    "            # pw\n",
    "        self.conv1 = nn.Conv2d(in_channels, hidden_dim, 1, 1, 0, bias = False)\n",
    "        self.bn1 = nn.BatchNorm2d(hidden_dim)\n",
    "        self.act1 = nn.ReLU(inplace = False)\n",
    "            # dw\n",
    "        self.conv2 = nn.Conv2d(hidden_dim, hidden_dim, 3, 2, 1, groups = hidden_dim, bias = False)\n",
    "        self.bn2 = nn.BatchNorm2d(hidden_dim)\n",
    "        self.act2 = nn.ReLU(inplace = False)\n",
    "            # pw-linear\n",
    "        self.conv3 = nn.Conv2d(hidden_dim, out_channels, 1, 1, 0, bias = False)\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels)\n",
    "        self.act3 = nn.ReLU(inplace = False)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        print(x.shape)\n",
    "        x = self.bn1(x)\n",
    "        print(x.shape)\n",
    "        x = self.act1(x)\n",
    "        print(x.shape)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        print(x.shape)\n",
    "        x = self.bn2(x)\n",
    "        print(x.shape)\n",
    "        x = self.act2(x)\n",
    "        print(x.shape)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        print(x.shape)\n",
    "        x = self.bn3(x)\n",
    "        print(x.shape)\n",
    "        x = self.act3(x)\n",
    "        print(x.shape)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_extras():\n",
    "    extras_layers = nn.ModuleList([\n",
    "#         Conv(1280, 1280, stride = 2), \n",
    "        liteConv(1280, 512, stride = 2, ),\n",
    "        liteConv(512, 256, stride = 2, ),\n",
    "        liteConv(256, 256, stride = 2, ),\n",
    "        liteConv(256, 64, stride = 2, )\n",
    "    ])\n",
    "    return extras_layers\n",
    "# add_extras()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multibox(n_classes, width_mult = 1.0, ):\n",
    "    \n",
    "    loc_layers = nn.ModuleList([\n",
    "        SeperableConv2d(in_channels = round(576 * width_mult),\n",
    "                        out_channels = 6 * 4,\n",
    "                        kernel_size = 3, \n",
    "                        padding = 1),\n",
    "        SeperableConv2d(in_channels = 1280, out_channels = 6 * 4, kernel_size = 3, padding = 1),\n",
    "        SeperableConv2d(in_channels = 512, out_channels = 6 * 4, kernel_size = 3, padding = 1),\n",
    "        SeperableConv2d(in_channels = 256, out_channels = 6 * 4, kernel_size = 3, padding = 1),\n",
    "        SeperableConv2d(in_channels = 256, out_channels = 6 * 4, kernel_size = 3, padding = 1),\n",
    "        nn.Conv2d(in_channels = 64, out_channels = 6 * 4, kernel_size = 1),\n",
    "    ])\n",
    "    \n",
    "    \n",
    "    conf_layers = nn.ModuleList([\n",
    "        SeperableConv2d(in_channels = round(576 * width_mult),\n",
    "                        out_channels = 6 * n_classes, \n",
    "                        kernel_size = 3, \n",
    "                        padding = 1),\n",
    "        SeperableConv2d(in_channels = 1280, out_channels = 6 * n_classes, kernel_size = 3, padding = 1),\n",
    "        SeperableConv2d(in_channels = 512, out_channels = 6 * n_classes, kernel_size = 3, padding = 1),\n",
    "        SeperableConv2d(in_channels = 256, out_channels = 6 * n_classes, kernel_size = 3, padding = 1),\n",
    "        SeperableConv2d(in_channels = 256, out_channels = 6 * n_classes, kernel_size = 3, padding = 1),\n",
    "        nn.Conv2d(in_channels = 64, out_channels = 6 * n_classes, kernel_size = 1),\n",
    "    ])\t\n",
    "    return loc_layers, conf_layers\n",
    "    \n",
    "# multibox(80, 1.0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "GraphPath = namedtuple(\"GraphPath\", ['s0', 'name', 's1'])  #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[GraphPath(s0=14, name='conv', s1=3), 19]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_layer_indexes = [\n",
    "    GraphPath(14, 'conv', 3),\n",
    "    19,\n",
    "]\n",
    "source_layer_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ssd(nn.Module):\n",
    "    '''single shot multibox architecture\n",
    "       backbone network is mobilenetv2\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 image_size,\n",
    "                 base,\n",
    "                 extras_layers,\n",
    "                 loc_layers,\n",
    "                 conf_layers,\n",
    "                 source_layer_index,\n",
    "                 n_classes,\n",
    "                 dropout_ratio = 0.1):\n",
    "        super(ssd, self).__init__()       \n",
    "        '''\n",
    "        Args:\n",
    "            image_size (int): size of input image size\n",
    "            base (nn.Module): backbone network - MobileNetV2 extract feature part\n",
    "            extras_layers (nn.ModuleList): extra layers that feed to multibox loc and conf layers\n",
    "            loc_layers (nn.ModuleList): bounding box output layer\n",
    "            conf_layers (nn.ModuleList): class confidence output layer\n",
    "            n_classes (int): number of class need to detect\n",
    "            dropout_ratio (float): percentage of dropout ratio\n",
    "        '''\n",
    "        self.n_classes = n_classes\n",
    "        self.image_size = image_size\n",
    "        self.base = base\n",
    "        self.source_layer_indexes = source_layer_index\n",
    "        self.extras_layers = extras_layers\n",
    "        self.dropout = nn.Dropout(p = dropout_ratio, inplace = False)\n",
    "        self.loc_layers = loc_layers\n",
    "        self.conf_layers = conf_layers\n",
    "        \n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        loc = []\n",
    "        conf = []\n",
    "        source = []\n",
    "        start_layer_index = 0\n",
    "        header_index = 0\n",
    "        for end_layer_index in self.source_layer_indexes:\n",
    "            \n",
    "            if isinstance(end_layer_index, GraphPath):\n",
    "                extras_from_base = end_layer_index\n",
    "                end_layer_index = end_layer_index.s0\n",
    "                added_layer = None\n",
    "            elif isinstance(end_layer_index, tuple):\n",
    "                added_layer = end_layer_index[1]\n",
    "                end_layer_index = end_layer_index[0]\n",
    "                extras_from_base = None\n",
    "            else:\n",
    "                added_layer = None\n",
    "                extras_from_base = None\n",
    "            \n",
    "            for layer in self.base[start_layer_index: end_layer_index]:\n",
    "                x = layer(x)\n",
    "                \n",
    "            if added_layer:\n",
    "                y = added_layer(x)\n",
    "            else:\n",
    "                y = x\n",
    "                \n",
    "            if extras_from_base:\n",
    "                sub = getattr(self.base[end_layer_index], extras_from_base.name)\n",
    "                \n",
    "                for layer in sub[:extras_from_base.s1]:\n",
    "                    x = layer(x)\n",
    "                    \n",
    "                y = x\n",
    "                \n",
    "                for layer in sub[extras_from_base.s1:]:\n",
    "                    x = layer(x)\n",
    "                    \n",
    "                end_layer_index += 1\n",
    "                \n",
    "            start_layer_index = end_layer_index\n",
    "            source.append(y)\n",
    "        \n",
    "        for layer in self.base[end_layer_index:]:\n",
    "            x = layer(x)\n",
    "        \n",
    "        for i,layer in enumerate(self.extras_layers):\n",
    "#             print(f\"index in extras layers {i}, {x.shape}\")\n",
    "            x = layer(x)\n",
    "#             print(\"feature map shape after extras:\",x.shape )\n",
    "            source.append(x)\n",
    "        \n",
    "        \n",
    "        for (x, l, c) in zip(source, self.loc_layers, self.conf_layers):    \n",
    "#             print(x.shape)\n",
    "            loc.append( l(x).permute(0, 2, 3, 1).contiguous())\n",
    "            conf.append( c(x).permute(0, 2, 3, 1).contiguous())\n",
    "        \n",
    "        loc = torch.cat([o.view(o.size(0), -1) for o in loc], 1)\n",
    "        conf = torch.cat([o.view(o.size(0), -1) for o in conf], 1)\n",
    "       \n",
    "        output = (\n",
    "            loc.view(loc.size(0), - 1, 4), \n",
    "#             torch.argmax(self.softmax(conf.view(conf.size(0), -1, self.n_classes)), dim = 2)\n",
    "            conf.view(conf.size(0), -1, self.n_classes)\n",
    "        )\n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image_size = 512\n",
    "extras_layer = add_extras()\n",
    "loc_layers, conf_layers = multibox(80, 1.0)\n",
    "n_classes = 80\n",
    "\n",
    "ssd_module = ssd(image_size, model.extract_feature, extras_layer, loc_layers, conf_layers, source_layer_indexes, n_classes)\n",
    "# torch.save(ssd_module, \"./test_ssd.h5\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "summary(ssd_module, (3,512,512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 576, 32, 32])\n",
      "torch.Size([6, 1280, 16, 16])\n",
      "torch.Size([6, 512, 8, 8])\n",
      "torch.Size([6, 256, 4, 4])\n",
      "torch.Size([6, 256, 2, 2])\n",
      "torch.Size([6, 64, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "# x = torch.rand((2,3,512,512))\n",
    "# o = ssd_module.forward(x)\n",
    "# summary(ssd_module, (3,512,512))\n",
    "\n",
    "inference = ssd_module(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 8190, 4])\n",
      "torch.Size([6, 8190, 80])\n"
     ]
    }
   ],
   "source": [
    "print(inference[0].shape)\n",
    "print(inference[1].shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import onnx\n",
    "output_path = \"test_mobilenetv2_ssd.onnx\"\n",
    "x = torch.rand((1,3,512, 512), requires_grad = True)\n",
    "torch.onnx.export(ssd_module, \n",
    "                  x,\n",
    "                  output_path,\n",
    "                  export_params = True,\n",
    "                  opset_version = 8,\n",
    "                  do_constant_folding = True,                     \n",
    "                  input_names = ['input'],\n",
    "                  output_names= ['coordinates', 'class'], \n",
    "                  dynamic_axes = { 'input': {0: 'batch_size'},\n",
    "                                   'output': {0: 'batch_size'}  },\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prior bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product as product\n",
    "from math import sqrt\n",
    "class PriorBox(object):\n",
    "    \"\"\"Compute priorbox coordinates in center-offset form for each source\n",
    "    feature map.\n",
    "    Note:\n",
    "    This 'layer' has changed between versions of the original SSD\n",
    "    paper, so we include both versions, but note v2 is the most tested and most\n",
    "    recent version of the paper.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg):\n",
    "        super(PriorBox, self).__init__()\n",
    "        self.image_size = cfg['min_dim']\n",
    "        # number of priors for feature map location (either 4 or 6)\n",
    "        self.num_priors = len(cfg['aspect_ratios'])\n",
    "        self.variance = cfg['variance'] or [0.1]\n",
    "        self.feature_maps = cfg['feature_maps']\n",
    "        self.min_sizes = cfg['min_sizes']\n",
    "        self.max_sizes = cfg['max_sizes']\n",
    "        self.steps = cfg['steps']\n",
    "        self.aspect_ratios = cfg['aspect_ratios']\n",
    "        self.clip = cfg['clip']\n",
    "        for v in self.variance:\n",
    "            if v <= 0:\n",
    "                raise ValueError('Variances must be greater than 0')\n",
    "\n",
    "    def forward(self):\n",
    "        mean = []\n",
    "        for k, f in enumerate(self.feature_maps):\n",
    "            for i, j in product(range(f), repeat=2):\n",
    "                f_k = self.image_size / self.steps[k]\n",
    "                cx = (j + 0.5) / f_k \n",
    "                cy = (i + 0.5) / f_k\n",
    "\n",
    "                s_k = self.min_sizes[k] / self.image_size\n",
    "                mean += [cx, cy, s_k, s_k]\n",
    "\n",
    "                # aspect_ratio: 1\n",
    "                # rel size: sqrt(s_k * s_(k+1))\n",
    "                if self.max_sizes:\n",
    "                    s_k_prime = sqrt(s_k * (self.max_sizes[k] / self.image_size))\n",
    "                    mean += [cx, cy, s_k_prime, s_k_prime]\n",
    "\n",
    "                # rest of aspect ratios\n",
    "                for ar in self.aspect_ratios[k]:\n",
    "                    mean += [cx, cy, s_k * sqrt(ar), s_k / sqrt(ar)]\n",
    "                    mean += [cx, cy, s_k / sqrt(ar), s_k * sqrt(ar)]\n",
    "\n",
    "        # back to torch land\n",
    "        output = torch.Tensor(mean).view(-1, 4)\n",
    "        if self.clip:\n",
    "            output.clamp_(max=1, min=0)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MOBILEV2_512 = {\n",
    "    \"feature_maps\": [32, 16, 8, 4, 2, 1],\n",
    "    \"min_dim\": 512,\n",
    "    \"steps\": [16, 32, 64, 128, 256, 512],\n",
    "    \"min_sizes\": [102.4,  174.08, 245.76, 317.44, 389.12, 460.8],\n",
    "    \"max_sizes\": [174.08, 245.76, 317.44, 389.12, 460.8,  512  ],\n",
    "    \"aspect_ratios\": [[2,3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3]],\n",
    "    \"variance\": [0.1, 0.2],\n",
    "    \"clip\": True,\n",
    "}    \n",
    "\n",
    "priorbox = PriorBox(MOBILEV2_512)\n",
    "priors = priorbox.forward()\n",
    "print(priors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multibox loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_form(boxes):\n",
    "    \"\"\" Convert prior_boxes to (xmin, ymin, xmax, ymax)\n",
    "    representation for comparison to point form ground truth data.\n",
    "    Args:\n",
    "        boxes: (tensor) center-size default boxes from priorbox layers.\n",
    "    Return:\n",
    "        boxes: (tensor) Converted xmin, ymin, xmax, ymax form of boxes.\n",
    "    \"\"\"\n",
    "    return torch.cat((boxes[:, :2] - boxes[:, 2:]/2,     # xmin, ymin\n",
    "                     boxes[:, :2] + boxes[:, 2:]/2), 1)  # xmax, ymax\n",
    "\n",
    "\n",
    "def center_size(boxes):\n",
    "    \"\"\" Convert prior_boxes to (cx, cy, w, h)\n",
    "    representation for comparison to center-size form ground truth data.\n",
    "    Args:\n",
    "        boxes: (tensor) point_form boxes\n",
    "    Return:\n",
    "        boxes: (tensor) Converted xmin, ymin, xmax, ymax form of boxes.\n",
    "    \"\"\"\n",
    "    return torch.cat((boxes[:, 2:] + boxes[:, :2])/2,  # cx, cy\n",
    "                     boxes[:, 2:] - boxes[:, :2], 1)  # w, h\n",
    "\n",
    "\n",
    "def intersect(box_a, box_b):\n",
    "    \"\"\" We resize both tensors to [A,B,2] without new malloc:\n",
    "    [A,2] -> [A,1,2] -> [A,B,2]\n",
    "    [B,2] -> [1,B,2] -> [A,B,2]\n",
    "    Then we compute the area of intersect between box_a and box_b.\n",
    "    Args:\n",
    "      box_a: (tensor) bounding boxes, Shape: [A,4].\n",
    "      box_b: (tensor) bounding boxes, Shape: [B,4].\n",
    "    Return:\n",
    "      (tensor) intersection area, Shape: [A,B].\n",
    "    \"\"\"\n",
    "    A = box_a.size(0)\n",
    "    B = box_b.size(0)\n",
    "    max_xy = torch.min(box_a[:, 2:].unsqueeze(1).expand(A, B, 2),\n",
    "                       box_b[:, 2:].unsqueeze(0).expand(A, B, 2))\n",
    "    min_xy = torch.max(box_a[:, :2].unsqueeze(1).expand(A, B, 2),\n",
    "                       box_b[:, :2].unsqueeze(0).expand(A, B, 2))\n",
    "    inter = torch.clamp((max_xy - min_xy), min=0)\n",
    "    return inter[:, :, 0] * inter[:, :, 1]\n",
    "\n",
    "\n",
    "def jaccard(box_a, box_b):\n",
    "    \"\"\"Compute the jaccard overlap of two sets of boxes.  The jaccard overlap\n",
    "    is simply the intersection over union of two boxes.  Here we operate on\n",
    "    ground truth boxes and default boxes.\n",
    "    E.g.:\n",
    "        A ∩ B / A ∪ B = A ∩ B / (area(A) + area(B) - A ∩ B)\n",
    "    Args:\n",
    "        box_a: (tensor) Ground truth bounding boxes, Shape: [num_objects,4]\n",
    "        box_b: (tensor) Prior boxes from priorbox layers, Shape: [num_priors,4]\n",
    "    Return:\n",
    "        jaccard overlap: (tensor) Shape: [box_a.size(0), box_b.size(0)]\n",
    "    \"\"\"\n",
    "    inter = intersect(box_a, box_b)\n",
    "    area_a = ((box_a[:, 2]-box_a[:, 0]) *\n",
    "              (box_a[:, 3]-box_a[:, 1])).unsqueeze(1).expand_as(inter)  # [A,B]\n",
    "    area_b = ((box_b[:, 2]-box_b[:, 0]) *\n",
    "              (box_b[:, 3]-box_b[:, 1])).unsqueeze(0).expand_as(inter)  # [A,B]\n",
    "    union = area_a + area_b - inter\n",
    "    return inter / union  # [A,B]\n",
    "\n",
    "\n",
    "\n",
    "def match(threshold, truths, priors, variances, labels, loc_t, conf_t, idx):\n",
    "    \"\"\"Match each prior box with the ground truth box of the highest jaccard\n",
    "    overlap, encode the bounding boxes, then return the matched indices\n",
    "    corresponding to both confidence and location preds.\n",
    "    Args:\n",
    "        threshold: (float) The overlap threshold used when mathing boxes.\n",
    "        truths: (tensor) Ground truth boxes, Shape: [num_obj, num_priors].\n",
    "        priors: (tensor) Prior boxes from priorbox layers, Shape: [n_priors,4].\n",
    "        variances: (tensor) Variances corresponding to each prior coord,\n",
    "            Shape: [num_priors, 4].\n",
    "        labels: (tensor) All the class labels for the image, Shape: [num_obj].\n",
    "        loc_t: (tensor) Tensor to be filled w/ endcoded location targets.\n",
    "        conf_t: (tensor) Tensor to be filled w/ matched indices for conf preds.\n",
    "        idx: (int) current batch index\n",
    "    Return:\n",
    "        The matched indices corresponding to 1)location and 2)confidence preds.\n",
    "    \"\"\"\n",
    "    # jaccard index\n",
    "    overlaps = jaccard(\n",
    "        truths,\n",
    "        point_form(priors)\n",
    "    )\n",
    "    # (Bipartite Matching)\n",
    "    # [1,num_objects] best prior for each ground truth\n",
    "    best_prior_overlap, best_prior_idx = overlaps.max(1, keepdim=True)\n",
    "\n",
    "    # [1,num_priors] best ground truth for each prior\n",
    "    best_truth_overlap, best_truth_idx = overlaps.max(0, keepdim=True)\n",
    "\n",
    "    best_truth_idx.squeeze_(0)\n",
    "    best_truth_overlap.squeeze_(0)\n",
    "    best_prior_idx.squeeze_(1)\n",
    "    best_prior_overlap.squeeze_(1)\n",
    "    best_truth_overlap.index_fill_(0, best_prior_idx, 2)  # ensure best prior\n",
    "\n",
    "    # TODO refactor: index  best_prior_idx with long tensor\n",
    "    # ensure every gt matches with its prior of max overlap\n",
    "    for j in range(best_prior_idx.size(0)):\n",
    "        best_truth_idx[best_prior_idx[j]] = j\n",
    "\n",
    "    matches = truths[best_truth_idx]          # Shape: [num_priors,4]\n",
    "    conf = labels[best_truth_idx] + 1         # Shape: [num_priors]\n",
    "    conf[best_truth_overlap < threshold] = 0  # label as background\n",
    "    loc = encode(matches, priors, variances)\n",
    "    loc_t[idx] = loc    # [num_priors,4] encoded offsets to learn\n",
    "    conf_t[idx] = conf  # [num_priors] top class label for each prior\n",
    "\n",
    "\n",
    "\n",
    "def encode(matched, priors, variances):\n",
    "    \"\"\"Encode the variances from the priorbox layers into the ground truth boxes\n",
    "    we have matched (based on jaccard overlap) with the prior boxes.\n",
    "    Args:\n",
    "        matched: (tensor) Coords of ground truth for each prior in point-form\n",
    "            Shape: [num_priors, 4].\n",
    "        priors: (tensor) Prior boxes in center-offset form\n",
    "            Shape: [num_priors,4].\n",
    "        variances: (list[float]) Variances of priorboxes\n",
    "    Return:\n",
    "        encoded boxes (tensor), Shape: [num_priors, 4]\n",
    "    \"\"\"\n",
    "\n",
    "    # dist b/t match center and prior's center\n",
    "    g_cxcy = (matched[:, :2] + matched[:, 2:])/2 - priors[:, :2]\n",
    "    # encode variance\n",
    "    g_cxcy /= (variances[0] * priors[:, 2:])\n",
    "    # match wh / prior wh\n",
    "    g_wh = (matched[:, 2:] - matched[:, :2]) / priors[:, 2:]\n",
    "    g_wh = torch.log(g_wh) / variances[1]\n",
    "    # return target for smooth_l1_loss\n",
    "    return torch.cat([g_cxcy, g_wh], 1)  # [num_priors,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_sum_exp(x):\n",
    "    \"\"\"Utility function for computing log_sum_exp while determining\n",
    "    This will be used to determine unaveraged confidence loss across\n",
    "    all examples in a batch.\n",
    "    Args:\n",
    "        x (Variable(tensor)): conf_preds from conf layers\n",
    "    \"\"\"\n",
    "    x_max = x.data.max()\n",
    "    return torch.log( torch.sum( torch.exp(x - x_max), 1, keepdim = True)) + x_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "class MultiBoxLoss(nn.Module):\n",
    "    \"\"\"SSD Weighted Loss Function\n",
    "    Compute Targets:\n",
    "        1) Produce Confidence Target Indices by matching  ground truth boxes\n",
    "           with (default) 'priorboxes' that have jaccard index > threshold parameter\n",
    "           (default threshold: 0.5).\n",
    "        2) Produce localization target by 'encoding' variance into offsets of ground\n",
    "           truth boxes and their matched  'priorboxes'.\n",
    "        3) Hard negative mining to filter the excessive number of negative examples\n",
    "           that comes with using a large number of default bounding boxes.\n",
    "           (default negative:positive ratio 3:1)\n",
    "    Objective Loss:\n",
    "        L(x,c,l,g) = (Lconf(x, c) + αLloc(x,l,g)) / N\n",
    "        Where, Lconf is the CrossEntropy Loss and Lloc is the SmoothL1 Loss\n",
    "        weighted by α which is set to 1 by cross val.\n",
    "        Args:\n",
    "            c: class confidences,\n",
    "            l: predicted boxes,\n",
    "            g: ground truth boxes\n",
    "            N: number of matched default boxes\n",
    "        See: https://arxiv.org/pdf/1512.02325.pdf for more details.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 n_classes,\n",
    "                 overlap_thresh = 0.5,\n",
    "                 prior_for_matching = True,\n",
    "                 bkg_label = 0,\n",
    "                 neg_mining = True,\n",
    "                 neg_pos = 3,\n",
    "                 neg_overlap = 0.5,\n",
    "                 encode_target = False):\n",
    "        super(MultiBoxLoss, self).__init__()\n",
    "        self.n_classes = n_classes\n",
    "        self.threshold = overlap_thresh\n",
    "        self.background_label = bkg_label\n",
    "        self.encode_target = encode_target\n",
    "        self.use_prior_for_matching  = prior_for_matching\n",
    "        self.do_neg_mining = neg_mining\n",
    "        self.negpos_ratio = neg_pos\n",
    "        self.neg_overlap = neg_overlap\n",
    "        self.variance = [0.1,0.2]\n",
    "        \n",
    "    def forward(self, predictions, priors, targets):\n",
    "        \"\"\"Multibox Loss\n",
    "        Args:\n",
    "            predictions (tuple): A tuple containing loc preds, conf preds,\n",
    "            and prior boxes from SSD net. (loc, conf)\n",
    "            \n",
    "            conf    (tensor): with shape (batch_size, num_priors, num_classes)\n",
    "            loc     (tensor): with shape (batch_size, num_priors, 4)\n",
    "            priors  (tensor): with shape (num_priors, 4)\n",
    "            targets (tensor): Ground truth boxes and labels, with shape \n",
    "                              (batch_size, num_objects, 5), last index store\n",
    "                              [xmin, ymin, xmax, ymax, label]\n",
    "        \"\"\"\n",
    "\n",
    "        loc_data, conf_data = predictions\n",
    "        priors = priors\n",
    "        num_batch = loc_data.size(0)\n",
    "        num_priors = (priors.size(0))\n",
    "        n_classes = self.n_classes\n",
    "\n",
    "        # match priors (default boxes) and ground truth boxes\n",
    "        loc_t = torch.Tensor(num_batch, num_priors, 4)\n",
    "        conf_t = torch.LongTensor(num_batch, num_priors)\n",
    "        for idx in range(num_batch):\n",
    "            ground_true_bboxes = targets[idx][:, :-1].data\n",
    "            labels = targets[idx][:, -1].data\n",
    "            defaults = priors.data\n",
    "            match(self.threshold,\n",
    "                  ground_true_bboxes,\n",
    "                  defaults,\n",
    "                  self.variance,\n",
    "                  labels,\n",
    "                  loc_t,\n",
    "                  conf_t,\n",
    "                  idx)\n",
    "   \n",
    "        # wrap targets\n",
    "        loc_t = Variable(loc_t, requires_grad = False)\n",
    "        conf_t = Variable(conf_t, requires_grad = False)\n",
    "\n",
    "        \n",
    "        pos= conf_t > 0\n",
    "\n",
    "        # Localization Loss (Smooth L1)\n",
    "        # Shape: [batch,num_priors,4]\n",
    "        pos_idx = pos.unsqueeze(pos.dim()).expand_as(loc_data)\n",
    "        loc_p = loc_data[pos_idx].view(-1, 4)\n",
    "        loc_t = loc_t[pos_idx].view(-1, 4)\n",
    "        loss_l = F.smooth_l1_loss(loc_p, loc_t, size_average = False)\n",
    "\n",
    "        # Compute max conf across batch for hard negative mining\n",
    "        batch_conf = conf_data.view(-1, self.n_classes)\n",
    "        loss_c = log_sum_exp(batch_conf) - batch_conf.gather(1, conf_t.view(-1, 1))\n",
    "\n",
    "        # Hard Negative Mining\n",
    "        loss_c = loss_c.view( pos.size()[0], pos.size()[1])\n",
    "        loss_c[pos] = 0 # filter out pos boxes for now\n",
    "        loss_c = loss_c.view(num_batch, -1)\n",
    "        _,loss_idx = loss_c.sort(1, descending = True)\n",
    "        _,idx_rank = loss_idx.sort(1)\n",
    "        num_pos = pos.long().sum(1, keepdim = True)\n",
    "        num_neg = torch.clamp( self.negpos_ratio * num_pos, max = pos.size(1) - 1)\n",
    "        neg = idx_rank < num_neg.expand_as(idx_rank)\n",
    "\n",
    "        # Confidence Loss Including Positive and Negative Examples\n",
    "        pos_idx = pos.unsqueeze(2).expand_as(conf_data)\n",
    "        neg_idx = neg.unsqueeze(2).expand_as(conf_data)\n",
    "        conf_p = conf_data[(pos_idx + neg_idx).gt(0)].view(-1, self.n_classes)\n",
    "        targets_weighted = conf_t[(pos + neg).gt(0)]\n",
    "        loss_c = F.cross_entropy(conf_p, targets_weighted, size_average = False)\n",
    "\n",
    "        # Sum of losses: L(x,c,l,g) = (Lconf(x, c) + αLloc(x,l,g)) / N\n",
    "\n",
    "        N = num_pos.data.sum().float()\n",
    "        loss_l = loss_l / N\n",
    "        loss_c /= N\n",
    "        return loss_l,loss_c\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = MultiBoxLoss(n_classes = 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 2\n",
    "lr = 0.01\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr= lr,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img, target in loader:\n",
    "    print(\"image shape: {}\".format(img.shape))\n",
    "#     print(\"label shape: {}\".format(target.shape))\n",
    "    for t in target:\n",
    "        print(\"sub target shape:\", t.shape)\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infernece = ssd_module(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(infernece[0].shape)\n",
    "print(infernece[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion(infernece, priors, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_priors = priors.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8190, 4)"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_priors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./priors.dat\", \"wb\") as binfile:\n",
    "    binfile.write(np_priors.tobytes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch13",
   "language": "python",
   "name": "pytorch13"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
